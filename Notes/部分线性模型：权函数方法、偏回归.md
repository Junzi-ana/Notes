---
{}
---
#SmallEssays/回归分析 

在笔记中[[非参数回归：回归的原理与最优化]]介绍了一些非参数回归的方法，这些方法所针对的是非参数模型
$$
Y_{i}=f(X_{i})+\varepsilon _{i},\quad i=1, \cdots, n
$$
然而当样本的维度较高或者有一些复杂数据的特性时，用上面介绍的非参数方法强行估计 $f$ 效果会非常差。事实上，对于一个现实问题，我们简单粗暴地假设其具有某种线性依赖关系是有失偏颇的，但抛弃一切假设地数据驱动同样不合适。为此，人们提出了一种部分线性模型
$$
Y_{i}=\beta ^{T}X_{i}+g(U_{i})+\varepsilon _{i},\quad i=1, \cdots, n
$$
它是一种半参数模型，协变量的一部分可以被自变量 $X$ 线性地解释，而另一部分难以解释的则用非参数方法拟合。这种做法既保留了非参数方法的灵活性，又很好的利用了现实问题的内在结构，极大的提升了效率。

有两种殊途同归的办法解决这个问题，其本质上都是想先解决其中的一个变量，从而化成熟悉的线性回归或者非参数回归问题。以下总假设 $U_{i}\in[0,1]$，$\mathbb{E}(\varepsilon _{i}|X_{i},U_{i})=0$ 。

第一种办法是消除 $X$，先作非参数回归。即作移项
$$
Y_{i}-\beta ^{T}X_{i}=g(U_{i})+\varepsilon _{i},\quad i=1, \cdots, n
$$
再将整个等式左端，对于给定的 $\beta$ 视作是协变量，再对 $U$ 作非参数回归。此处简单选用权函数估计，得到
$$
\hat{g}(u;\beta)=\sum^{n}_{i=1}W_{ni}(u)(Y_{i}-\beta ^{T}X_{i}) 
$$
再将 $\hat{g}$ 的估计代入 $g$，用最小二乘法求解线性回归问题
$$
\hat{\beta}=\underset{\beta}{\text{argmin}}\; \sum^{n}_{i=1}\big(Y_{i}-\beta ^{T}X_{i}-\hat{g}(U_{i},\beta)\big) ^{2}
$$

引入记号
$$
\begin{align}
\hat{g}_{1}(u)&=\widehat{\mathbb{E}}(y|U=u)=\sum^{n}_{i=1}W_{ni}(u)Y_{i} \\
\hat{g}_{2}(u)&=\widehat{\mathbb{E}}(X|U=u)=\sum^{n}_{i=1}W_{ni}(u)X_{i} 
\end{align}
$$
并记
$$
\widetilde{X}_{i}=X_{i}-\hat{g}_{2}(u_{i}),\quad \widetilde{Y}_{i}=Y_{i}-\hat{g}(u_{i})
$$
则由[[平方和分解式：线性回归|线性回归]]的知识，得到
$$
\hat{\beta}=(\widetilde{X}^{T}\widetilde{X})^{-1}\widetilde{X}^{T}\widetilde{Y}
$$

第二种方法是先消除 $U$，即两边对 $U$ 取条件期望，得
$$
g(U_{i})=g_{1}(U_{i})-\beta ^{T}g_{2}(U_{i})
$$
其中 $g_{1}(u)=\mathbb{E}(Y|U=u),~g_{2}(u)=\mathbb{E}(X|U=u)$ 。代入得
$$
Y_{i}-g_{1}(U_{i})=\beta ^{T}\big( X_{i}-g_{2}(U_{i}) \big) +\varepsilon _{i}
$$
同样用线性回归的方法先解出 $\hat{\beta}(g_{1},g_{2})$，再将 $\hat{g}_{1},~\hat{g}_{2}$ 代入得到记得 $\hat{\beta}$ 和 $\hat{g}$ 。两种方法所得的结果是相同的。

相比之下，第二种方法能够比较直觉地引出 $\hat{g}_{1},~\hat{g}_{2}$，而第一种方法在用最小二乘法求解 $\hat{\beta}$ 时会相对复杂些。现在再来考虑这两个记号的具体含义，事实上，我们将 $X,~Y$ 中关于 $U$ 的信息全部封装在了 $\hat{g}_{1},~\hat{g}_{2}$ 中，再减去它们，得到了直觉上与 $U$ 无关的 $\widetilde{X},~\widetilde{Y}$ ，从而可以简单地作线性回归。

那么，从这个角度继续引申，假设 $K$ 是一个光滑矩阵，直觉上它包含了所有关于 $U$ 的信息，那么记
$$
\widetilde{X}=(I-K)X,\quad \widetilde{Y}=(I-K)Y
$$
则得到了**偏回归估计**
$$
\hat{\beta}_{PR}=(\widetilde{X}^{T}\widetilde{X})^{-1}\widetilde{X}^{T}\widetilde{Y}
$$
特别地，当 $K$ 是对称幂等矩阵时
$$
\begin{align}
\hat{\beta}_{PR}&=(X^{T}(I-K)X)^{-1}X(I-K)Y\\
\hat{g}_{PR}&=K(Y-X\hat{\beta}_{PR})
\end{align}
$$

在上面权函数估计的情景下
$$
\widetilde{Y}=\begin{bmatrix}
Y_{1} \\
\vdots \\
Y_{n}
\end{bmatrix}-\begin{bmatrix}
W_{n1}(U_{1}) & \cdots & W_{nn}(U_{1}) \\
\vdots & \ddots & \vdots \\
W_{n1}(U_{1}) & \cdots & W_{nn}(U_{n})
\end{bmatrix}\begin{bmatrix}
Y_{1} \\
\vdots \\
Y_{n}
\end{bmatrix}=(I-K)Y
$$
于是有
$$
K=\begin{bmatrix}
W_{n1}(U_{1}) & \cdots & W_{nn}(U_{1}) \\
\vdots & \ddots & \vdots \\
W_{n1}(U_{1}) & \cdots & W_{nn}(U_{n})
\end{bmatrix}
$$

最后简要地给出统计量 $\hat{\beta}_{PR}$ 的一些统计性质而略去有关条件及证明。

**定理** 在一定的条件下，记 $n^{-1}\widetilde{X}^{T}\widetilde{X}\to V$，则成立渐近正态性
$$
n^{-\frac{1}{2}}(\hat{\beta}_{PR}-\mathbb{E}\hat{\beta}_{PR})\xrightarrow{d}N(0,\sigma^{2}V^{-1}) 
$$
---
*References:*
1. *现代统计模型, 薛留根*
2. [[回归分析与线性模型第三章.pdf]]

---
*2025-11-19*