---
{}
---
#SmallEssays/回归分析 

在笔记[[非参数回归：经验似然方法]]中介绍的经验似然方法同样可以应用于[[部分线性模型：权函数方法、偏回归|部分线性模型]]。我们先确定其估计方程。沿用前文记号，得分函数应当为
$$
Z(X,Y;\beta)=(X-g_{2})\big( Y-g_{1}-\beta ^{T}(X-g_{2}) \big) 
$$
从而估计方程为 $\sum^{n}_{i=1}p_{i}Z_{i}=0$ 。然而 $g_{1},~g_{2}$ 未知，只能先用权函数估计，用 $\hat{g}_{1},~\hat{g}_{2}$ 代替之，从而用
$$
\widehat{Z}(\widetilde{X},\widetilde{Y};\beta)=\widetilde{X}(\widetilde{Y}-\beta ^{T}\widetilde{X})
$$
代替 $Z$，于是经验似然比为
$$
R(\beta)=\max\{ \prod^{n}_{i=1}np_{i} : \sum^{n}_{i=1} p_{i}=0,~ \sum^{n}_{i=1} p_{i}\widehat{Z}_{i}(\beta)=0,~ p_{i}\geq 0\}
$$
在原本的 Wilks 定理中，我们只有一个参数 $\theta$，没有其它的估计量了，然而在这里，我们还对原本的估计方程作了一次估计，这样 Wilks 定理还能成立吗？

考虑到在 Wilks 定理中，$-2\ln R(\theta)$ 有 $S_{Z}^{T}H^{-1}_{Z}S_{Z}+o_{p}(1)$ 的形式，其中
$$
S_{Z}=\frac{1}{\sqrt{ n }}\sum^{n}_{i=1}Z_{i}(\theta),\quad H_{Z} =\frac{1}{n}\sum^{n}_{i=1}Z_{i}(\theta)Z_{i}(\theta) ^{T}
$$
分别是标准化得分和样本信息阵。于是我们只需要证明 $S_{\widehat{Z}}=S_{Z}+o_{p}(1)$ 和 $H_{\widehat{Z}}=H_{Z}+o_{p}(1)$ 。为此，令估计余项为
$$
r_{i}=\widehat{Z}_{i}(\theta)-Z_{i}(\theta)
$$
要证明 $\frac{1}{\sqrt{ n }}\sum^{n}_{i=1}r_{i}=o_{p}(1)$ ，一般来说可以利用 Chebyshev 不等式。假如估计得当，$\mathbb{E}r_{i}=0$ 且 $i\neq j$ 时 $\mathbb{E}r_{i}r_{j}=0$，就有
$$
\mathbb{E}\left[ \frac{1}{\sqrt{ n }}\sum^{n}_{i=1}r_{i}  \right] ^{2}=\frac{1}{n}\sum^{n}_{i=1} \mathbb{E}r_{i}^{2} 
$$
于是只要估计 $\mathbb{E}r^{2}_{n}=o_{p}(1)$ (在 $\widehat{Z}$ 中包含了对 $g$ 的估计，因而是依赖于 $n$ 和 $h$ 的)。这个过程会用到关于 N-W 估计的偏差和方差的结论。

当然，如果 $r_{i}$ 不是零均值或者前后有相关性，这个估计过程会变得更加艰难，结果一般不是 $\chi^{2}$ 分布。比如数据有删失，$\widehat{Z}_{i}(\theta)$ 一般不是 $Z_{i}(\theta )$ 的无偏估计。

---
*2025-11-27*