#SmallEssays/统计学习 

考虑如下的混合 Gauss 模型
$$
X\sim \sum^{K}_{k=1}\pi_{k}N(\mu_{k},~ \Sigma_{k}) 
$$
即每个数据都是取自 $K$ 个独立的 Gauss 总体中的某一个，并且每个 Gauss 总体的权重 $\pi_{k}$ 和参数 $\mu_{k},~\Sigma_{k}$ 未知。现在需要估计总共 $3K$ 个参数。

给定样本 $x_{1}, \cdots,~ x_{n}$，假如要做极大似然估计，写出单个样本的[[数理统计基本思想：似然函数、似然比|似然函数]]为
$$
\begin{align}
L(x_{i}|\pi,\mu,\Sigma)&=\sum^{K}_{k=1}\pi_{k}p(x_{i}|\mu_{k},\Sigma_{k})\\
&\propto\sum^{K}_{k=1}  \pi_{k}|\Sigma_{k}|^{-\frac{1}{2}}\exp\left( (x_{i}-\mu_{k})^{T}\Sigma_{k}^{-1}(x_{i}-\mu_{k}) \right) 
\end{align}

$$
与正态总体的参数估计不同的是，在取对数之后，会出现形如 $\mathscr{l}=\ln(\sum^{K}_{k=1}\cdots)$ 的对数下求和的形式，对这种形式作最优化的难度会非常之大。

真正的困难在于这些数据是无标签的。为此，我们考虑引入**隐变量** $z_{i}=(z_{i1}, \cdots,~ z_{iK})$ 来表示数据的标签。其中 $z_{ik}$ 是 $\text{0-1}$ 变量，当且仅当样本 $x_{i}$ 由第 $k$ 个总体生成时取值为 $1$ 。

使用一个 $K$ 维向量而非单个从 $1$ 到 $K$ 取值的变量，最大的好处是可以直接带入到似然量中计算，否则引入这个隐变量的意义也不大了。此时，似然量可以写成
$$
L(x_{i},z_{i}|\pi,\mu,\Sigma)=\prod^{K}_{k=1}\big( \pi_{k}p(x|k) \big) ^{z_{ik}} 
$$
于是整个样本的对数似然就可以写成
$$
\mathscr{l}(x|\pi,\mu,\Sigma)=\sum^{n}_{i=1} \sum^{K}_{k=1}z_{ik}(\ln \pi_{k}+\ln p(x_{i}|\mu_{k},\Sigma_{k})) 
$$
既然写到了这一步，那我们不妨允许 $z_{i}$ 的每个分量在 $[0,1]$ 中取值，并将其改记为 $\gamma_{i}$ ，称为 $x_{i}$ 的**归属向量**，直观上用来表示 $x_{i}$ 属于第 $k$ 个总体的概率。

表面上看这只是一个**软化**的技巧，但在不知不觉间，这个问题已经产生了本质上的改变。因为当 $z_{ik}$ 只能取值 $0$ 或 $1$ 时，我们至始至终都只是在进行形式上的变换，并没有办法确定其取值；但当变成一个概率值后，我们就有办法不失偏颇地确定它的值了。取定先验概率 $\pi^{(0)}$ 和初值 $\mu^{(0)},~\Sigma^{(0)}$，使用 Bayes 公式
$$
\gamma_{ik}^{(1)}=\frac{\pi_{k}^{(0)}p(x_{i}|\mu_{k}^{(0)},\Sigma_{k}^{(0)})}{\sum^{K}_{j=1}\pi_{j}^{(0)} p(x_{i}|\mu_{j}^{(0)},\Sigma_{j}^{(0)}) }
$$
再将 $\gamma_{ik}^{(1)}$ 代入上式，通过最大化 $\mathscr{l}$，就能求得 $\hat{\pi}^{(1)},~\hat{\mu}^{(1)},~\widehat{\Sigma}^{(1)}$ 。将求得的 $\hat{\pi}^{(1)}$ 视为先验概率，重复上述过程，我们得到了序列 $\{ \hat{\pi}^{(n)},~\hat{\mu}^{(n)},~\widehat{\Sigma}^{(n)} \}_{1}^{\infty}$ 。

尽管我们归属向量 $\gamma_{i}^{(0)}$ 来代替了数据的真实标签 $z_{i}$ ，但我们相信，每次迭代之后，估计的效果能有所改善，即是 $\mathscr{l}$ 递增的，直到最后这个序列收敛于极大似然估计 $\hat{\pi}^{*},~\hat{\mu}^{*},~\widehat{\Sigma}^{*}$ 。这个求解 MLE 的算法称为 **EM 算法** 。碍于篇幅，其一般的含义与收敛性问题会在之后的笔记中给出。

---
*2025-11-25*