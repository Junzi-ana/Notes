---
{}
---
#SmallEssays/回归分析 

假定有两个变量 $x$ 和 $y$ ，其中 $y$ 一般是随机的，$x$ 可以是随机的也可以确定的。当观测到样本 $(x_{i},~y_{i})$ 时，我们希望找到 $x$ 和 $y$ 之间的某种关系。

为此建立回归模型
$$
y=f(x)+\varepsilon
$$
其中 $\varepsilon$ 为噪声，是随机变量，一般我们假定它满足 **Gauss-Markov 假设**，即 $\varepsilon_{i}$ 独立同分布且与 $x$ 无关，$\mathbb{E}\varepsilon=0$，$\text{Var}\,\varepsilon<\infty$ 。

上式两端对 $x$ 取条件期望，得 $\mathbb{E}\left[ y|x \right]=f(x)$ 。利用条件期望的性质，条件期望算子实质上是 $L^{2}$ 空间的投影算子，应当使得**均方误差** $\mathbb{E}\varepsilon^{2}=\mathbb{E}( y-\mathbb{E}\left[ y|x \right] )^{2}$ 达到最小值。假如**用均值代替期望**，得到
$$
f(x)=\underset{f}{\text{argmin}}\;\sum^{n}_{i=1} (y_{i}-f(x_{i}))^{2} 
$$
从数值的角度，则是使用**最小二乘法**对样本点 $(x_{i},~y_{i})$ 进行**曲线拟合**。

对任意函数 $f$ 作最优化并不容易，而且就算能解出来，也不一定能得到令人满意的结果。一般来说，需要假定 $f(x;\theta)$ 的形式已知，转而去对参数 $\theta$ 作最优化。

此时记**残差平方和**为
$$
Q(\theta)=\sum^{n}_{i=1} (y_{i}-f(x_{i};\theta))^{2} 
$$
在一定的正则条件下，要最优化 $Q(\theta)$，即得到方程组
$$
\frac{\partial Q}{\partial \theta^{k}} =-2 \sum^{n}_{i=1} (y_{i}-f(x_{i};\theta))\frac{\partial f}{\partial \theta^{k}} =0,\quad k=1,\cdots,d
$$
称为**正规方程组**，解出 $\hat{\theta}$ 即为参数 $\theta$ 的普通最小二乘估计，从而得到 $\hat{y}=f(x,\hat{\theta})$ 即为回归方程。

例如当 $f(x;\theta)$ 取作 $f(x;\theta)=\sum^{d}_{k=1}\theta_{k}\varphi_{k}(x)$，并记
$$
G=[\left< \varphi_{j},\varphi_{k} \right>]_{d\times d},\quad b=[\left< y,\varphi_{k} \right> ]_{1\times d}
$$
即得到正规方程组 $G\theta=b$ ，其中
$$
\left< \varphi_{j}, \varphi_{k} \right> =\sum^{n}_{i=1} \varphi_{j}(x_{i})\varphi_{k}(x_{i}),\quad \left< y,\varphi_{k} \right> =\sum^{n}_{i=1} y_{i}\varphi_{k}(x_{i})
$$

到了非参数的情形中，由于不能假定 $f$ 的全局参数形式，也不能做到对 $\mathbb{E}\varepsilon^{2}$ 的全局最优化。为此，我们退而求其次地对 $\mathbb{E}\varepsilon^{2}$ 进行局部最优化，即使得在每个 $z$ 的邻域内残差平方和最小化。此时，我们可以假定 $f(x)$ 在 $z$ 的邻域上有**局部参数形式** $f_{z}(x;\theta)$。

写出 $(z-h,z+h)$ 上的局部残差平方和的表达式
$$
Q_{z,h}(\theta)=\sum^{n}_{i=1} (y_{i}-f_{z}(x_{i},\theta))^{2}\mathbb{1}_{(z-h,z+h)}(x_{i}) 
$$
当然正如[[非参数估计：核函数方法]]中所述，示性函数的性质不太好，用核函数来表示领域是一个更合适的做法，并无妨将 $z$ 改记为 $x$ ，得到
$$
Q_{x,h}(\theta)=\sum^{n}_{i=1} (y_{i}-f_{x}(x_{i},\theta))^{2}K_{h}(x-x_{i}) 
$$
其中 $K_{h}(u)=\frac{1}{h}K(\frac{u}{h})$ 。

指定 $h$，对每个 $x$ 作最优化，即求得 $\hat{\theta}(x)=\underset{\theta \in \mathbb{R}^{d}}{\text{argmin}}\;Q_{x,h}(\theta)$，得到回归曲线为 $\hat{f}(x)=f_{x}(x;\hat{\theta}(x))$ 。

特别地，当假定 $f_{z}(x;\theta)$ 在每个 $z$ 附近为常数，即 $f_{z}(x;\theta)\equiv\theta$，则得到 **N-W 估计**
$$
\hat{f}^{NW}(x)=\underset{\theta}{\text{argmin}}\;\sum^{n}_{i=1} (y_{i}-\theta)^{2}K_{h}(x-x_{i}) 
$$
其解为 $y_{i}$ 的线性加权形式
$$
\hat{f}^{NW}(x)=\sum^{n}_{i=1} W^{K}_{ni}(x)y_{i}
$$
其中 $W^{K}_{ni}(x)=\frac{K_{h}(x-x_{i})}{\sum^{n}_{i=1}K_{h}(x-x_{i})}$ 。因此 N-W 估计又称**权函数估计**。在一点 $x$ 处，其偏差和方差满足
$$
\begin{align}
\text{bias}(\hat{f}^{NW})&=\mathbb{E}(\hat{f}^{NW}(x)-f(x))=O(h^{2})\\
\text{Var}\,(\hat{f}^{NW})&=O(\frac{1}{nh})
\end{align}
$$

当然也有其它常用的参数形式，例如取
$$
f_{z}(x;\theta)=\sum^{\mathscr{l}}_{k=0} \theta_{k}\varphi_{k}(\frac{x-z}{h})
$$
$\varphi_{k}$ 是基函数，可以选为多项式函数、插值基函数、正交基函数，分别得到不同的估计。值得一提的是选取 $\varphi_{k}(u)=\frac{u^{k}}{k!}h^{k}$ 时，得到的 $\hat{\varphi}_{0}(x)$ 是 $f(x)$ 的估计，而 $\hat{\varphi}_{k}(x)$ 是 $f^{(k)}(x)$ 的估计，称为**局部多项式估计**。

---
*Reference: [[回归分析与线性模型第二章.pdf]]*

---
*2025-10-27*
