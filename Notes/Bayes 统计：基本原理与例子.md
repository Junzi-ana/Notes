---
{}
---
#SmallEssays/统计学习 

过往熟知的统计理论，预设总体属于某一分布族 $\{ F_{\theta} \}_{\theta \in\Theta}$，待到抽取样本 $x_{1}, ~ x_{2}, \cdots,~ x_{n}$ 后再对待估参数 $\theta$ 作出统计推断。频率学派认为 $\theta$ 是一个确定的值，样本与待估参数通过[[数理统计基本思想：似然函数、似然比|似然量]] $L(\theta;x)$ 联系起来，由此产生的估计方法称为**极大似然估计**；而 Bayes 学派从 Bayes 公式的角度出发，认为 $\theta$ 是一个随机变量，具有**先验分布** $p(\theta)$，当获得样本之后，对参数的认识用**后验分布** $p(\theta|x)$ 表示，由此产生的估计方法称为**最大后验概率估计**。

Bayes 统计最大的争议在于先验分布的确定往往是主观的，甚至有时是随意的。尽管这一点在实践上带来了很大的灵活性，但在理论上始终难以服众。在展开进一步论述之前，先用一些例子来简短地说明 Bayes 统计的处理方式。

设样本 $x_{1}, ~ x_{2}, \cdots,~ x_{n}$，由 Bayes 公式，后验概率为
$$
p(\theta|x)=\frac{p(x|\theta)p(\theta)}{p(x)}
$$
由于 $p(x)$ 与 $\theta$ 无关，因此和我们的优化目标 $p(\theta|x)$ 无关，在 Bayes 统计中我们常采用比例符号来省略这些无关的常数项。从而可以写成
$$
p(\theta|x)\propto p(x|\theta)p(\theta)=\prod^{n}_{i=1}p(x_{j}|\theta)p(\theta) 
$$
尽管独立性在实践中是比较苛刻的假设，但若没有这个假设，在实践中就要估计 $n$ 个样本之间的联合分布 $\hat{p}(x|\theta)$，这需要极大的样本量，是不可行的。因此在统计学习中，作出样本之间条件独立假设的方法又称为**朴素 Bayes** 方法。然而在统计学中对样本作出独立性假设是比较自然的。

**例 1** 设总体 $X$ 服从成功率为 $\theta$ 的二项分布，$\theta$ 服从先验分布 $B(\alpha,\beta)$，从中抽取 $n$ 个样本，估计 $\theta$ 。

**分析** 由 Bayes 公式
$$
\begin{align}
p(\theta|x)&\propto \prod^{n}_{i=1}\theta^{x_{i}}(1-\theta)^{1-x_{i}}\cdot\theta^{\alpha-1}(1-\theta)^{\beta-1} \\
&\propto\theta^{n_{1}+\alpha-1}(1-\theta)^{n_{0}+\beta-1}
\end{align}
$$
其中 $n_{1}$ 是成功的次数，$n_{0}$ 是失败的次数。由此看出在给定样本 $x$ 的条件下，$\theta$ 的后验分布为 $B(n_{1}+\alpha,n_{0}+\beta)$，仍然是一个 Beta 分布。这样保持后验分布形式不变的先验分布称为 $X$ 的**共轭先验分布**，在没有其他更合适的选择时，使用这个分布可以起到简化计算的作用。

选取使得后验分布 $p(\theta|x)$ 达到最大的点，得到
$$
\hat{\theta}_{MAP}=\frac{n_{1}+\alpha-1}{n+\alpha+\beta-2}
$$
与之对比的是极大似然估计
$$
\hat{\theta}_{MLE}=\frac{n_{1}}{n}
$$
可以看出，当 $\alpha=\beta=1$ 时二者相等，即 MLE 可以视为先验分布为 $B(1,1)$ ，即均匀分布时的 MAP；另一方面，MAP 也可以视为在样本中额外添加 $\alpha-1$ 个成功样本和 $\beta-1$ 个失败样本时的 MLE。对此我们可以作出两个重要观察。
1. 频率统计有时可被视为隐含了某种先验假定的 Bayes 统计
2. Bayes 统计的先验假设有时也隐含着某种频率解释

**例 2** 假设总体 $X$ 服从正态分布 $N(\mu,~\sigma^{2})$ ，假设 $\sigma^{2}$ 已知，$\mu$ 有先验分布 $N(\mu_{0}, \tau^{2})$ ，估计 $\mu$ 。 ^1a3092

**分析** 由 Bayes 公式，计算后验概率
$$
\begin{align}
p(\mu|x)&\propto \exp \left( -\frac{n}{2\sigma^{2}}(\mu-\bar{x})^{2} \right)\cdot \exp \left(- \frac{1}{2\tau^{2}}(\mu-\mu_{0})^{2} \right) \\
{\scriptsize(\text{complete square})}&\propto \exp\left( -\frac{\sigma^{2}+n\tau^{2}}{2\sigma^{2}\tau^{2} }\Big(\mu-  \frac{n\bar{x}\tau^{2}+\mu_{0}\sigma^{2}}{\sigma^{2}+n\tau^{2}} \Big) ^{2} \right) 
\end{align}
$$
于是有后验分布
$$
\mu|x \sim N\Big(  \frac{\tau^{2}}{\tau^{2}+\frac{\sigma^{2}}{n}}\bar{x}+ \frac{\frac{\sigma^{2}}{n}}{\tau^{2}+\frac{\sigma^{2}}{n}}\mu_{0} ,~ \frac{1}{\frac{n}{\sigma^{2}}+\frac{1}{\tau^{2}}}\Big) 
$$
这个式子看上去比较复杂，但仔细观察后并不难理解。首先是均值，实际上是 $\bar{x}$ 与 $\mu_{0}$ 的线性加权，权重与方差的倒数，即**精度**成正比。当 $\tau$ 小，说明先验分布比较坚定，此时后验分布仍然要在比较大程度上迁就先验分布，当 $n$ 大则说明数据支撑比较充分，此时先验应该让步于样本均值。同样借助精度这一术语，也可以直接看出**后验分布的精度为先验精度与样本精度的直接加和**。

有趣的是，依照频率统计的框架， $\bar{x}$ 是 $\mu$ 的 UMVUE，在所有无偏估计中 $\bar{x}$ 的方差达到了最小，然而在 Bayes 统计的框架下，在叠加上先验分布之后，其精度自然会大于 $\bar{x}$ 。然而代价是，这样得到的估计量也不一定是无偏的了。

最后，尽管 Bayes 统计在统计学领域一度饱受争议，但它在机器学习领域却大放异彩，这源于其为**在线学习**提供了框架。比方说，在面对分批次的数据时，频率学派先学得了 $\hat{\theta}(\mathcal{D}_{1})$ ，再学 $\hat{\theta}(\mathcal{D}_{1},\mathcal{D}_{2})$ 时，只能把 $\mathcal{D}_{1},~\mathcal{D}_{2}$ 当成一个完整的数据集重新学一遍，此前学得的 $\hat{\theta}(\mathcal{D}_{1})$ 便再不起作用，效率比较低下；而 Bayes 学派可以在学得 $\hat{\theta}(\mathcal{D}_{1})$，之后，可以将其作为一个新的先验分布，再计算关于 $\mathcal{D}_{2}$ 的后验分布，这样可以大大提高数据的利用效率。

---
*Reference: 统计学习方法, 李航*

---
*2025-11-20*