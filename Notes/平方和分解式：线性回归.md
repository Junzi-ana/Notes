#SmallEssays/多元统计分析 

在笔记[[平方和分解式：方差分析]]中，介绍了平方和分解式
$$
SST=SSW+SSB
$$
的多种形式及在方差应用，本篇笔记会讨论其在回归分析中的应用。

考虑一元线性回归
$$
y_{i}=\beta ^{T}X_{i}+\varepsilon _{i},\quad i=1, \cdots, n
$$
其中 $X_{i}$ 是 $p+1$ 维列向量，其第一个元素为 $1$，并记
$$
X=\begin{bmatrix}
X_{1} & X_{2} & \cdots & X_{n}
\end{bmatrix}^{T},\quad y=\begin{bmatrix}
y_{1} & y_{2} & \cdots & y_{n}
\end{bmatrix}^{T}
$$
$X$ 的每一行是一个样本，称为设计矩阵。于是可以将回归问题记为
$$
y=X\beta+\varepsilon
$$

使用最小二乘法得到
$$
\begin{align}
\beta =\underset{\beta }{\text{argmin}}\;(y-X\beta )^{T}(y-X\beta )
\end{align}

$$
求偏导数或者利用线性代数技巧都可求得
$$
\hat{\beta} =(X^{T}X)^{-}X^{T}y,\quad \widehat{y}=X(X^{T}X)^{-}X^{T}y 
$$
记 $\mathcal{H}_{X}=X(X^{T}X)^{-}X^{T}$ ，其中 $\mathcal{H}_{X}$ 称为**帽子矩阵**，因为在 $\mathcal{H}_{X}$ 的作用下，$y$ 变成了 $\widehat{y}$，外观上像是戴上了一顶帽子。

> [!note]-
> $A^{-}$ 称为矩阵的广义逆，表示矩阵方程 $AXA=A$ 的任一解，其中存在唯一特殊的解 $A^{+}$ 称为 **Moore-Penrose 广义逆**，其满足一些特殊的性质。若在上式中取 MP 广义逆，则有 $X^{+}=(X^{T}X)^{+}X^{T}$，于是 $\hat{\beta}=X^{+}\beta$，是所有 $\hat{\beta}$ 中模长最小的。
> 
> 此处单纯出于简化记号的目的，总是记 $\hat{\beta}=X^{+}y$ 。

现在希望对拟合的结果作检验，即作假设检验
$$
H_{0}:\beta_{1}=\cdots=\beta_{p}=0\quad \leftrightarrow \quad H_{1}: \exists\;j,~ \beta_{j}\neq0
$$
从方差分析的角度，若 $H_{0}$ 为真，此时 $y_{i}$ 不再受 $X_{i}$ 的影响，每个 $y_{i}$ 都应具有相同的均值，其取值上的差异仅仅来自于随机噪声 $\varepsilon_{i}$ 。所以我们将 $y_{1},\cdots,y_{n}$ 视作是分别来自于 $n$ 个独立的均值为 $\mathbb{E}y_{i}=\hat{y}_{i}$，方差为 $\sigma^{2}$ 的正态总体的样本。

此时，问题被化为 $n$ 组，每组 $1$ 个样本的方差分析问题，从而有平方和分解式
$$
\sum^{n}_{i=1}(y_{i}-\bar{y})^{2}=\sum^{n}_{i=1}(y_{i}-\hat{y}_{i})^{2}+\sum^{n}_{i=1}(\hat{y}_{i}-\bar{y})^{2}
$$
在回归中， $\sum^{n}_{i=1}(y_{i}-\bar{y})^{2}$ 仍记作 $SST$ ，称为**总平方和**；$\sum^{n}_{i=1}(y_{i}-\hat{y}_{i})^{2}$ 记作 $SSR$ ，称为**回归平方和**，对应于方差分析中的 $SSW$；$\sum^{n}_{i=1}(\hat{y}_{i}-\bar{y}_{i})^{2}$ 记作 $SSE$ ，称为**残差平方和**，对应于方差分析中的 $SSB$ 。

接下来我们计算自由度分配。此处 $\hat{y}_{i}$ 是通过 $p+1$ 个参数估计出来的，应占用 $p+1$ 个自由度，因此 $SSR\sim \chi^{2}_{n-p-1}$， $SSE\sim \chi^{2}_{p}$，且它们相互独立。于是定义检验统计量
$$
F=\frac{SSR / p}{SSE / (n-p-1)}
$$
当 $H_{0}$ 为真时，$F\sim F_{p,n-p-1}$，此为回归分析中的 $F$ 检验。

> [!remark]-
> 当 $X$ 不列满秩时，自由度消耗为 $r=\mathrm{rank}(XX^{T})$，应当用 $r$ 替换上文的 $p+1$，下文同理。在不加说明时可以假设 $X$ 是列满秩的。

到了多元线性回归，有 $q$ 个协变量和 $p$ 个自变量，从而多元线性回归模型被写成
$$
Y=XB+\varepsilon
$$
其中 $Y$ 是 $n\times q$ 矩阵，每一行是一个样本，$B$ 是 $(p+1)\times q$ 阶系数矩阵。

与一元线性回归类似，通过极小化残差平方和可得
$$
\widehat{B}=X^{+}Y,\quad \widehat{Y}=\mathcal{H}_{X}Y
$$
类似地也有离差阵的分解式（注意到 $H_{X}\overline{Y}=\overline{Y}$）
$$
\sum^{n}_{i=1}(Y_{i}-\overline{Y})(Y_{i}-\overline{Y})^{T}=\sum^{n}_{i=1}(Y_{i}-\widehat{Y}_{i})(Y_{i}-\widehat{Y}_{i})^{T}+\sum^{n}_{i=1}(\widehat{Y}_{i}-\overline{Y})(\widehat{Y}_{i}-\overline{Y})^{T}
$$
以及对应的 Wilks 检验。

不过在回归分析中，尽管也有平方和分解式，也可以类似于方差分析的方法推导对应的假设检验，但这个记号有时却显得不那么趁手。一方面，有时我们希望将截距项 $\beta_{0}$ 与斜率 $\beta_{1},\cdots,\beta _{n}$ 分开讨论，另一方面，从方差分析的角度导出的 $F$ 检验只适用于检验整个模型，有时候我们希望检验的是某几个自变量是否起到作用，或者某几个因变量是否得到了很好的解释。因此，需要将线性回归问题嵌入到[[线性模型：估计与假设检验|线性模型]]的框架下讨论，同时采用一些新的记号来阐释。但无论如何，从方差分析和平方和分解的角度来看线性回归无疑是有启发性的，同时它们又统一于线性模型。

---
*Reference: 应用回归分析, 何晓群*

---
*2025-11-14*