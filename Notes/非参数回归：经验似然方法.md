---
{}
---
#SmallEssays/数理统计 

在笔记[[非参数估计：数理统计基本定理]]中，我们用经验分布来推断未知分布。更进一步，我们还可以用经验分布来推断一些总体的数字特征，例如总体均值 $\mu$。假如直接用经验分布 $\widehat{F}_{n}$ 来代替 $F$，那么直接求期望得到估计 $\hat{\mu}=\bar{X}$ 。这种估计的好处是不依赖对总体分布的假定，但坏处也是很明显的，就是我们不知道此时这个估计 $\hat{\mu}$ 的好坏，也不知道如何对它作假设检验。

想要导出非参数情形的假设检验，我们回想参数情形的假设检验是如何导出的。一个重要的假设检验手段是[[数理统计基本思想：似然函数、似然比|似然比检验]]。这一想法固然可取，然而，在非参数情形下，就连写出似然函数都成一个问题。这该如何是好呢？解铃还须系铃人，**经验似然方法的手段就是，直接用经验分布估计真实分布**，反正由数理统计基本定理，二者之间不会相差很远。

假定样本取值为 $z_{1}, ~ z_{2}, \cdots,~ z_{n}$ ，经验分布函数为 $\widehat{F}_{n}$ 。由于 $\widehat{F}_{n}$ 的形式已知，而且是离散的，它被自动地参数化为了离散分布族 ${F}_{p}=\displaystyle \left[ \begin{matrix} z_{1}&z_{2}&\cdots&z_{n} \\p_{1}&p_{2}&\cdots&p_{n}\end{matrix}  \right]$ 中的一个元素，参数空间为
$$
\Theta=\{ (p_{1}, \cdots,~ p_{n}) : p_{i}\geq 0,~ \sum^{n}_{i=1} p_{i} =1 \}
$$
其中，当 $p=(\frac{1}{n},\cdots,\frac{1}{n})$ 时，$F_{p}$ 即为 $\widehat{F}_{n}$ 。

接下来要确定 $\Theta_{0}$ ，这取决于我们作出的零假设，或者说我们要估计的东西。比如我们想估计总体均值 $\mu$ ，那么设定矩条件 $\mathbb{E}\left[ X-\mu \right]=0$，从而估计方程 $\sum^{n}_{i=1}p_{i}(z_{i}-\mu)=0$ ；一般而言，要估计参数 $\theta$，设定矩条件 $\mathbb{E}\left[ g(X;\theta) \right]=0$ ，估计方程为
$$
\sum^{n}_{i=1}p_{i}g(z_{i};\theta)=0
$$
例如在一元线性回归中，矩条件就应该为 $\mathbb{E}\left[ (y-\beta x)x \right]=0$ 。

接下来构造似然函数。$F_{p}$ 的似然函数为 $L(F_{p})=\prod^{n}_{i=1}p_{i}$ ，特别地 $\widehat{F}_{n}$ 是 $\{ z_{1}, ~ z_{2}, \cdots,~ z_{n} \}$ 上的均匀分布，故 $L(\widehat{F}_{n})=(\frac{1}{n})^{n}$ 。

因此，可以类似地构造似然比
$$
\lambda=\frac{\sup_{p \in\Theta}L(F_{p})}{\sup_{p \in\Theta_{0}}L(F_{p})}
$$
对于分子，因为我们已经将 $\widehat{F}_{n}$ 当成真值了，它理所应当地使 $L(F_{p})$ 最大化，而分母则要我们在 $\Theta_{0}$ 上去对 $L(F_{p})$ 作一个最优化。为此，我们写出
$$
R(\theta)=\max\left\{ \prod^{n}_{i=1}np_{i} : p\in\Theta_{0}  \right\}
$$
其中待估参数 $\theta$ 出现在估计方程中。且 $\ln\lambda=-\ln R(\theta)$ 。

$\ln\lambda$ 的意义也是明显的，我们可以计算
$$
\ln \frac{L(\widehat{F}_{n})}{L(F_{p})} =\sum^{n}_{i=1} \left( \frac{1}{n} \right)\ln \left( \frac{1/n}{p_{i}} \right)=D_{KL}(\widehat{F}_{n}\|F_{p})  
$$
其中 $D_{KL}(P\|Q)$ 为 $Q$ 到 $P$ 的 [[EM 算法：变分推断与 ELBO|KL 散度]]，表示用 $Q$ 分布估计 $P$ 分布时，平均损失的信息量。因此，$\ln\lambda$ 可以衡量，当零假设成立时（即在估计方程的约束下）$F_{p}$ 对经验分布 $\widehat{F}_{n}$ 的近似程度，当 $\ln\lambda$ 越大时，说明距离经验分布越远，即倾向于拒绝零假设。关于假设之间的距离，详见笔记[[假设检验的基本原理、测度之间的距离]]。 ^a086b4

与在[[似然比检验的 Wilks 定理]]中类似地，在经验似然方法中也有 Wilks 定理，即在一定的条件下，$2\ln\lambda$ 或是 $-2\ln R(\theta_{0})$ 渐近地服从卡方分布 $\chi^{2}_{r}$ ，其中 $r=\mathrm{rank}~\Sigma_{0}$，$\Sigma_{0}$ 为真实分布 $F$ 的协方差矩阵。这个定理的证明极其复杂，在笔记[[经验似然方法的 Wilks 定理]]中对其思想做了简单介绍。

作为总结，在使用经验似然方法构造置信域时，有如下特点：
1. 不需要构造枢轴统计量
2. 所得置信域具有域不变性
3. 置信域的形状完全由数据自行确定（Data Driven）。

---
*2025-10-22*
